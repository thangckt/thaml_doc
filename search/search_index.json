{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"thaml Documentation","text":""},{"location":"#thaml","title":"<code>thaml</code>","text":"<p>The package for LLM applications</p>"},{"location":"chat/","title":"chat","text":""},{"location":"chat/#thaml.chat","title":"<code>thaml.chat</code>","text":"<p>This module contains the chatbot classes and methods to work with text generation models.</p>"},{"location":"chat/#thaml.chat.ChatOpenAI","title":"<code>ChatOpenAI(base_url='local_copilot', api_key: str = None, model='gpt-4', temperature=0.5, top_p=1, max_tokens=8200, stream=False, system_prompt: str = None)</code>","text":"<p>             Bases: <code>ChatBase</code></p> <p>Class for chatbot using OpenAI API via <code>openai</code> package</p> <p>Parameters:</p> <ul> <li> <code>base_url</code>             (<code>str</code>, default:                 <code>'local_copilot'</code> )         \u2013          <p>The OpenAI API base URL.</p> </li> <li> <code>api_key</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The OpenAI API key.</p> </li> <li> <code>model</code>             (<code>str</code>, default:                 <code>'gpt-4'</code> )         \u2013          <p>The model to use for the chat client. All models can be found at the OpenAI site. But in Copilot, there may only 2 models: <code>gpt-4</code> and `gpt-3'</p> </li> <li> <code>temperature</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>The temperature to use for the chat client. The temperature is a value between 0 and 1. Lower temperatures will cause the model to repeat itself more often, while higher temperatures will increase the model's diversity of responses. Use either <code>temperature</code> or <code>top_p</code>, but not both.</p> </li> <li> <code>top_p</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>An alternative to sampling with temperature. The top_p is a value between 0 and 1. Use either <code>temperature</code> or <code>top_p</code>, but not both.</p> </li> <li> <code>max_tokens</code>             (<code>int</code>, default:                 <code>8200</code> )         \u2013          <p>The maximum number of tokens to generate in the response.</p> </li> <li> <code>stream</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to stream the response or not.</p> </li> <li> <code>system_prompt</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The prompt to use for the system.</p> </li> </ul>"},{"location":"chat/#thaml.chat.ChatOpenAI.private_urls","title":"<code>private_urls = private_urls</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatOpenAI.params","title":"<code>params = {'base_url': base_url, 'api_key': api_key, 'model': model, 'temperature': temperature, 'top_p': top_p, 'max_tokens': max_tokens, 'stream': stream, 'system_prompt': system_prompt, 'chat_history': []}</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatOpenAI.save_history","title":"<code>save_history(prompt, response)</code>","text":""},{"location":"chat/#thaml.chat.ChatOpenAI.export_history","title":"<code>export_history(filename='chat_history.txt')</code>","text":""},{"location":"chat/#thaml.chat.ChatOpenAI.load_history","title":"<code>load_history(filename='chat_history.txt')</code>","text":""},{"location":"chat/#thaml.chat.ChatOpenAI.ask","title":"<code>ask(prompt='hello', save_history=False, use_history=False) -&gt; str</code>","text":"<p>Ask GPT-4 a question and return the answer. Use new openai API</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>             (<code>str</code>, default:                 <code>'hello'</code> )         \u2013          <p>The question to ask GPT-4.</p> </li> <li> <code>save_history</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to save the question and answer to the chat history.</p> </li> <li> <code>use_history</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use the chat history in the current request.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (            <code>str</code> )        \u2013          <p>The answer to the question.</p> </li> </ul>"},{"location":"chat/#thaml.chat.ChatPost","title":"<code>ChatPost(base_url='local_copilot', api_key: str = None, model='gpt-4', temperature=0.5, top_p=1, max_tokens=8200, stream=False, system_prompt: str = None)</code>","text":"<p>             Bases: <code>ChatBase</code></p> <p>Class for chatbot using OpenAI API via <code>requests</code> package</p> Quote <ul> <li>https://www.techasoft.com/post/how-to-use-chatgpt-api-in-python-for-your-real-time-data)</li> <li>Curl vs python's requests: https://stackoverflow.com/questions/31061227/curl-vs-python-requests-when-hitting-apis</li> <li>Python and REST APIs: Interacting With Web Services: https://realpython.com/api-integration-in-python/</li> <li>Asynchronous Requests in Python: https://superfastpython.com/python-async-requests/</li> <li>Make request python faster: https://skillshats.com/blogs/optimize-python-requests-for-faster-performance/</li> </ul>"},{"location":"chat/#thaml.chat.ChatPost.private_urls","title":"<code>private_urls = private_urls</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatPost.params","title":"<code>params = {'base_url': base_url, 'api_key': api_key, 'model': model, 'temperature': temperature, 'top_p': top_p, 'max_tokens': max_tokens, 'stream': stream, 'system_prompt': system_prompt, 'chat_history': []}</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatPost.save_history","title":"<code>save_history(prompt, response)</code>","text":""},{"location":"chat/#thaml.chat.ChatPost.export_history","title":"<code>export_history(filename='chat_history.txt')</code>","text":""},{"location":"chat/#thaml.chat.ChatPost.load_history","title":"<code>load_history(filename='chat_history.txt')</code>","text":""},{"location":"chat/#thaml.chat.ChatPost.ask","title":"<code>ask(prompt='hello')</code>","text":""},{"location":"chat/#thaml.chat.ChatGoogle","title":"<code>ChatGoogle(api_key=None, model='gemini-pro', temperature=0.5, top_p=1, max_tokens=8200, stream=False, system_prompt='')</code>","text":"<p>Class for chatbot using Google's Gemini API via google.generativeai package</p>"},{"location":"chat/#thaml.chat.ChatGoogle.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatGoogle.params","title":"<code>params = {'api_key': api_key, 'model': model, 'temperature': temperature, 'top_p': top_p, 'max_tokens': max_tokens, 'stream': stream, 'system_prompt': system_prompt, 'chat_history': []}</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatGoogle.ask","title":"<code>ask(prompt='hello my friend')</code>","text":""},{"location":"chat/#thaml.chat.ChatFree","title":"<code>ChatFree(provider: str = None, api_key: str = None, model='gpt-4', temperature=0.5, top_p=1, max_tokens=8200, stream=False, system_prompt='')</code>","text":"<p>             Bases: <code>_Base</code></p> <p>Class for chatbot using reverse enginerring models. Use the new g4f version: <code>g4f.client.Client</code></p> <p>Parameters:</p> <ul> <li> <code>provider</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The provider of the model. If None, the best provider will be used. Defaults to None.</p> </li> <li> <code>api_key</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The API key for the provider. Defaults to None.</p> </li> <li> <code>model</code>             (<code>str</code>, default:                 <code>'gpt-4'</code> )         \u2013          <p>The model to use. Defaults to \"gpt-4\".</p> </li> <li> <code>temperature</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>The temperature of the model. Defaults to 0.5.</p> </li> <li> <code>top_p</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The top_p of the model. Defaults to 1.</p> </li> <li> <code>max_tokens</code>             (<code>int</code>, default:                 <code>8200</code> )         \u2013          <p>The max tokens of the model. Defaults to 8200.</p> </li> <li> <code>system_prompt</code>             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>The system prompt of the model. Defaults to \"\".</p> </li> </ul>"},{"location":"chat/#thaml.chat.ChatFree.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatFree.avail_providers","title":"<code>avail_providers = self._avail_providers()</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatFree.params","title":"<code>params = {'provider': provider, 'api_key': api_key, 'model': model, 'temperature': temperature, 'top_p': top_p, 'max_tokens': max_tokens, 'stream': stream, 'system_prompt': system_prompt, 'chat_history': []}</code>  <code>instance-attribute</code>","text":""},{"location":"chat/#thaml.chat.ChatFree.ask","title":"<code>ask(prompt: str) -&gt; str</code>","text":"<p>Ask the chatbot a question. Args:     prompt (str): The input string for the chatbot.</p> <p>Returns:</p> <ul> <li> <code>text</code> (            <code>str</code> )        \u2013          <p>The answer to the question.</p> </li> </ul>"},{"location":"chat/#thaml.chat.ChatOpenAIweb","title":"<code>ChatOpenAIweb(session_token: str = None, conversation_id: str = None)</code>","text":"<p>Class for chatbot using OpenAI API via Web interface</p> Example <pre><code>from thaml.chat import ChatOpenAIweb\nchatbot = ChatOpenAIweb()\nresponse = chatbot.ask(\"hello\")\nprint(response)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>session_token</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The session token from the cookies.</p> </li> <li> <code>conversation_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The conversation id to use.</p> </li> </ul>"},{"location":"chat/#thaml.chat.ChatOpenAIweb.ask","title":"<code>ask(prompt='hello') -&gt; str</code>","text":"<p>Ask a question and return the answer. Use new openai API</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>             (<code>str</code>, default:                 <code>'hello'</code> )         \u2013          <p>The question to ask GPT-4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (            <code>str</code> )        \u2013          <p>The answer to the question.</p> </li> </ul>"},{"location":"rag/","title":"rag","text":""},{"location":"rag/#thaml.rag","title":"<code>thaml.rag</code>","text":""},{"location":"rag/#thaml.rag.load_document","title":"<code>load_document(doc_path: str = '', ext: str = None) -&gt; list[Document]</code>","text":"<p>Load documents from the given path, using langchain's [document_loaders]. Supported file types: .pdf, .docx, .txt, .md, .lnk (Windows' shortcuts).</p> <p>Parameters:</p> <ul> <li> <code>doc_path</code>             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>The path to the folder containing the documents.</p> </li> <li> <code>ext</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The file extension of the documents to be loaded, e.g., '.pdf'. Default, loads all files in the folder.</p> </li> </ul> <p>Returns:     list[Document]: A list of Document objects, containing the loaded documents.</p>"},{"location":"utils/","title":"utils","text":""},{"location":"utils/#thaml.utils","title":"<code>thaml.utils</code>","text":""},{"location":"utils/#thaml.utils.html_diffs","title":"<code>html_diffs(a: str, b: str) -&gt; str</code>","text":"<p>Return the side-by-side HTML of the differences between text_a and text_b.</p> <p>Parameters:</p> <ul> <li> <code>a</code>             (<code>str</code>)         \u2013          <p>The first string.</p> </li> <li> <code>b</code>             (<code>str</code>)         \u2013          <p>The second string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>html</code> (            <code>str</code> )        \u2013          <p>The side-by-side HTML of the differences between text_a and text_b.</p> </li> </ul>"}]}